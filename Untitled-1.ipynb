{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6afc021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e62b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5587bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = joblib.load(model_path)\n",
    "    feature_cols = joblib.load(\"optimized_random_forest_model.pkl\")\n",
    "    return model, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5d140df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"merged_stock_data.csv\")\n",
    "model=load_model('optimized_random_forest_model.pkl')\n",
    "\n",
    "x_test = df.select_dtypes(include=['number']).drop(['future_return_5d'], axis=1, errors='ignore')\n",
    "y_test = df['trend_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b5233620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import load\n",
    "model = load(\"optimized_random_forest_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9dc9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"mlruns\")  # Local  directory\n",
    "mlflow.set_experiment(\"Stock Market Trend Prediction\")\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f884aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62673db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjoblib\u001b[49m.dump(\u001b[33m'\u001b[39m\u001b[33moptimized_random_forest_model.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m joblib.dump(X_train.columns.tolist(), \u001b[33m'\u001b[39m\u001b[33mfeature_columns.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump('optimized_random_forest_model.pkl')\n",
    "joblib.dump(X_train.columns.tolist(), 'feature_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab12aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = joblib.load(model_path)\n",
    "    feature_cols = joblib.load(\"feature_columns.pkl\")\n",
    "    return model, feature_cols\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load saved model from pickle file\"\"\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "with open(\"optimized_random_forest_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ef0caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    df =pd.read_csv(\"merged_stock_data.csv\")  # Adjust path as needed\n",
    "    \"\"\"Load your test data - adjust path as needed\"\"\"\n",
    "    # Replace with your actual test data loading logic\n",
    "    # This is just a placeholder structure\n",
    "    X_test = df.drop(['trend_label', 'future_return_5d'], axis=1).values  # Replace with actual test features\n",
    "    y_test = df['trend_label'].values  # Replace with actual test labels\n",
    "    return X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ed505fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = {\n",
    "    \n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "    \"f1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "    \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "    \"recall\": recall_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    return metrics,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10ef0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, filename):\n",
    "    \"\"\"Plot and save feature importance\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.title('Feature Importance')\n",
    "        plt.bar(range(len(importances)), importances[indices])\n",
    "        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0874a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db656964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0957dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_baseline_model():\n",
    "    \"\"\"Log Phase 1: Logistic Regression Baseline Model\"\"\"\n",
    "    print(\"Logging Baseline Logistic Regression Model...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"logistic_regression_baseline\"):\n",
    "        # Load your saved logistic regression model\n",
    "        try:\n",
    "            model = load_model('logistic_regression_model.pkl')  # Adjust path\n",
    "            model_path = 'logistic_regression_model.pkl'\n",
    "\n",
    "        except:\n",
    "            print(\"Warning: Could not load logistic_regression_model.pkl\")\n",
    "            print(\"Creating dummy model for demonstration...\")\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            # Fit with dummy data (replace with your actual training)\n",
    "            X_dummy = np.random.randn(100, 5)\n",
    "            y_dummy = np.random.randint(0, 2, 100)\n",
    "            model.fit(X_dummy, y_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e21e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "# Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53e7b318",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Open', 'High', 'Low', 'Close', 'Volume'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Define features and target\u001b[39;00m\n\u001b[32m      8\u001b[39m feature_cols = [\u001b[33m\"\u001b[39m\u001b[33mOpen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHigh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVolume\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# adjust to your dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m y = df[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# replace with your actual target column\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Split into train/test\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maryn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maryn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maryn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['Open', 'High', 'Low', 'Close', 'Volume'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"merged_stock_data.csv\")\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]  # adjust to your dataset\n",
    "X = df[feature_cols]\n",
    "y = df[\"target\"]  # replace with your actual target column\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
