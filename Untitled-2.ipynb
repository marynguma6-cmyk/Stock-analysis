{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cbe326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: LOADING AND PREPARING DATA\n",
      "============================================================\n",
      "âœ… Data loaded successfully!\n",
      "   Shape: (15502, 34)\n",
      "   Columns: 34\n",
      "\n",
      "First 5 rows:\n",
      "   ticker        date    open    high     low   close     volume  \\\n",
      "0  STK001  2021-01-04  158.09  160.97  158.09  160.11   962644.0   \n",
      "1  STK001  2021-01-05  163.16  165.50  160.76  162.36  1312685.0   \n",
      "2  STK001  2021-01-06  161.89  162.51  160.94  161.78  1449177.0   \n",
      "3  STK001  2021-01-07  163.33  167.90  163.33  167.07  1534833.0   \n",
      "4  STK001  2021-01-08  168.20  168.20  164.12  165.68   848261.0   \n",
      "\n",
      "   adjusted_close      sma_20      sma_50  ...  volume_ratio  momentum_10  \\\n",
      "0          160.11  160.110000  160.110000  ...      1.000000          NaN   \n",
      "1          162.36  161.235000  161.235000  ...      1.153842          NaN   \n",
      "2          161.78  161.416667  161.416667  ...      1.167277          NaN   \n",
      "3          167.07  162.830000  162.830000  ...      1.167320          NaN   \n",
      "4          165.68  163.400000  163.400000  ...      0.694431          NaN   \n",
      "\n",
      "   momentum_20  price_to_sma_50  volatility_20  future_return_5d  trend_label  \\\n",
      "0          NaN         0.000000            NaN          0.026357      Uptrend   \n",
      "1          NaN         0.006977            NaN          0.018169     Sideways   \n",
      "2          NaN         0.002251       0.012463         -0.012301     Sideways   \n",
      "3          NaN         0.026039       0.018138         -0.061950    Downtrend   \n",
      "4          NaN         0.013953       0.018663         -0.070196    Downtrend   \n",
      "\n",
      "   company_name      sector    ipo_date  \n",
      "0      TechCorp  Technology  2021-04-10  \n",
      "1      TechCorp  Technology  2021-04-10  \n",
      "2      TechCorp  Technology  2021-04-10  \n",
      "3      TechCorp  Technology  2021-04-10  \n",
      "4      TechCorp  Technology  2021-04-10  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "\n",
      "ðŸ“Š Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15502 entries, 0 to 15501\n",
      "Data columns (total 34 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   ticker            15502 non-null  object \n",
      " 1   date              15502 non-null  object \n",
      " 2   open              15502 non-null  float64\n",
      " 3   high              15502 non-null  float64\n",
      " 4   low               15502 non-null  float64\n",
      " 5   close             15502 non-null  float64\n",
      " 6   volume            15502 non-null  float64\n",
      " 7   adjusted_close    15502 non-null  float64\n",
      " 8   sma_20            15192 non-null  float64\n",
      " 9   sma_50            15502 non-null  float64\n",
      " 10  sma_200           15502 non-null  float64\n",
      " 11  ema_12            15502 non-null  float64\n",
      " 12  ema_26            15502 non-null  float64\n",
      " 13  macd              15191 non-null  float64\n",
      " 14  macd_signal       15502 non-null  float64\n",
      " 15  macd_histogram    15502 non-null  float64\n",
      " 16  rsi_14            15172 non-null  float64\n",
      " 17  bb_middle         15502 non-null  float64\n",
      " 18  bb_upper          15482 non-null  float64\n",
      " 19  bb_lower          15482 non-null  float64\n",
      " 20  bb_width          15171 non-null  float64\n",
      " 21  true_range        15502 non-null  float64\n",
      " 22  atr_14            15502 non-null  float64\n",
      " 23  volume_sma_20     15502 non-null  float64\n",
      " 24  volume_ratio      15192 non-null  float64\n",
      " 25  momentum_10       15302 non-null  float64\n",
      " 26  momentum_20       15102 non-null  float64\n",
      " 27  price_to_sma_50   15502 non-null  float64\n",
      " 28  volatility_20     15462 non-null  float64\n",
      " 29  future_return_5d  15502 non-null  float64\n",
      " 30  trend_label       15502 non-null  object \n",
      " 31  company_name      15502 non-null  object \n",
      " 32  sector            15502 non-null  object \n",
      " 33  ipo_date          15502 non-null  object \n",
      "dtypes: float64(28), object(6)\n",
      "memory usage: 4.0+ MB\n",
      "None\n",
      "\n",
      "ðŸ“ˆ Missing Values:\n",
      "momentum_20       400\n",
      "bb_width          331\n",
      "rsi_14            330\n",
      "macd              311\n",
      "sma_20            310\n",
      "volume_ratio      310\n",
      "momentum_10       200\n",
      "volatility_20      40\n",
      "bb_upper           20\n",
      "bb_lower           20\n",
      "date                0\n",
      "ticker              0\n",
      "sma_50              0\n",
      "open                0\n",
      "close               0\n",
      "volume              0\n",
      "low                 0\n",
      "high                0\n",
      "bb_middle           0\n",
      "macd_histogram      0\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "STEP 2: DATA CLEANING\n",
      "============================================================\n",
      "âœ… Dropped 'future_return_5d' column\n",
      "âœ… Removed rows with null trend_label\n",
      "   Rows removed: 0\n",
      "   Remaining rows: 15502\n",
      "\n",
      "ðŸ“Š Trend Label Distribution:\n",
      "trend_label\n",
      "Uptrend      5532\n",
      "Downtrend    5026\n",
      "Sideways     4944\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… Converted trend_label to binary target\n",
      "   Target distribution: {0: 15502}\n",
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE ENGINEERING\n",
      "============================================================\n",
      "âœ… Added technical indicators\n",
      "   New shape: (14771, 45)\n",
      "   New columns: 45\n",
      "\n",
      "============================================================\n",
      "STEP 4: FEATURE SELECTION\n",
      "============================================================\n",
      "âœ… Available features: 14/14\n",
      "Available features:\n",
      "   1. sma_200\n",
      "   2. atr_14\n",
      "   3. volume_sma_20\n",
      "   4. volatility_20\n",
      "   5. bb_width\n",
      "   6. rsi\n",
      "   7. macd\n",
      "   8. macd_hist\n",
      "   9. sma_50\n",
      "  10. ema_20\n",
      "  11. volume_ratio\n",
      "  12. returns\n",
      "  13. bb_upper\n",
      "  14. bb_lower\n",
      "\n",
      "ðŸ“Š Selected 34 features for modeling\n",
      "\n",
      "============================================================\n",
      "STEP 5: PREPARE TRAINING DATA\n",
      "============================================================\n",
      "Class Distribution:\n",
      "target\n",
      "0    14771\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Features shape: (14771, 34)\n",
      "Target shape: (14771,)\n",
      "\n",
      "Training set: (11816, 34)\n",
      "Testing set:  (2955, 34)\n",
      "\n",
      "============================================================\n",
      "STEP 6: HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "Performing GridSearchCV... (this may take a few minutes)\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "\n",
      "âœ… Grid search completed!\n",
      "Best parameters: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation score (F1-weighted): 1.0000\n",
      "\n",
      "============================================================\n",
      "STEP 7: MODEL EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 258\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m    257\u001b[39m y_pred = optimized_random_forest_model.predict(X_test)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m y_pred_proba = \u001b[43moptimized_random_forest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m    261\u001b[39m accuracy = accuracy_score(y_test, y_pred)\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STOCK PREDICTION MODEL - COMPLETE WORKFLOW\n",
    "# =============================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, classification_report, confusion_matrix,\n",
    "                           roc_auc_score, roc_curve)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2. Load and Prepare Data\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: LOADING AND PREPARING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load your merged stock data\n",
    "try:\n",
    "    df = pd.read_csv('merged_stock_data.csv')\n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ File 'merged_stock_data.csv' not found.\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'close': 100 * (1 + np.random.normal(0.0005, 0.02, n_samples)).cumprod(),\n",
    "        'volume': np.random.lognormal(14, 1.5, n_samples),\n",
    "        'trend_label': np.random.choice(['UP', 'DOWN', 'SIDEWAYS'], n_samples, p=[0.4, 0.3, 0.3])\n",
    "    })\n",
    "    \n",
    "    # Add some technical indicators\n",
    "    df['sma_200'] = df['close'].rolling(200).mean()\n",
    "    df['atr_14'] = (df['close'] - df['close'].shift(1)).abs().rolling(14).mean()\n",
    "    df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    df['future_return_5d'] = (df['close'].shift(-5) / df['close'] - 1) * 100\n",
    "\n",
    "# Check data info\n",
    "print(\"\\nðŸ“Š Data Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nðŸ“ˆ Missing Values:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. Data Cleaning\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: DATA CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Drop future_return_5d as requested\n",
    "if 'future_return_5d' in df.columns:\n",
    "    df = df.drop('future_return_5d', axis=1)\n",
    "    print(\"âœ… Dropped 'future_return_5d' column\")\n",
    "\n",
    "# Remove rows with null trend_label\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=['trend_label'])\n",
    "print(f\"âœ… Removed rows with null trend_label\")\n",
    "print(f\"   Rows removed: {initial_rows - len(df)}\")\n",
    "print(f\"   Remaining rows: {len(df)}\")\n",
    "\n",
    "# Check trend distribution\n",
    "print(\"\\nðŸ“Š Trend Label Distribution:\")\n",
    "print(df['trend_label'].value_counts())\n",
    "\n",
    "# Convert trend_label to binary if needed\n",
    "# Assuming UP = 1, others = 0 (adjust based on your needs)\n",
    "if df['trend_label'].dtype == 'object':\n",
    "    df['target'] = (df['trend_label'] == 'UP').astype(int)\n",
    "    print(\"\\nâœ… Converted trend_label to binary target\")\n",
    "    print(f\"   Target distribution: {df['target'].value_counts().to_dict()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 4. Feature Engineering (if not already in data)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate additional technical indicators if not present\n",
    "def calculate_indicators(data):\n",
    "    \"\"\"Calculate technical indicators\"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Price-based indicators\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility_20'] = df['returns'].rolling(20).std()\n",
    "    \n",
    "    # Moving averages\n",
    "    for period in [5, 10, 20, 50]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period).mean()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = exp1 - exp2\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Volume indicators\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    \n",
    "    # Drop NaN values from rolling calculations\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df = calculate_indicators(df)\n",
    "print(f\"âœ… Added technical indicators\")\n",
    "print(f\"   New shape: {df.shape}\")\n",
    "print(f\"   New columns: {len(df.columns)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5. Select Top Features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Based on your analysis, these are the top features\n",
    "top_features = [\n",
    "    'sma_200', 'atr_14', 'volume_sma_20', 'volatility_20',\n",
    "    'bb_width', 'rsi', 'macd', 'macd_hist',\n",
    "    'sma_50', 'ema_20', 'volume_ratio',\n",
    "    'returns', 'bb_upper', 'bb_lower'\n",
    "]\n",
    "\n",
    "# Check which features exist in our data\n",
    "available_features = [f for f in top_features if f in df.columns]\n",
    "print(f\"âœ… Available features: {len(available_features)}/{len(top_features)}\")\n",
    "print(\"Available features:\")\n",
    "for i, feature in enumerate(available_features, 1):\n",
    "    print(f\"  {i:2}. {feature}\")\n",
    "\n",
    "# Add more features if we have them (aiming for 34+ as you mentioned)\n",
    "all_numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "all_numeric_features = [f for f in all_numeric_features if f != 'target']\n",
    "\n",
    "# Select top 34 features (or all if less)\n",
    "if len(all_numeric_features) > 34:\n",
    "    # Use correlation with target to select features\n",
    "    correlations = df[all_numeric_features].apply(lambda x: x.corr(df['target'])).abs()\n",
    "    selected_features = correlations.nlargest(34).index.tolist()\n",
    "else:\n",
    "    selected_features = all_numeric_features\n",
    "\n",
    "print(f\"\\nðŸ“Š Selected {len(selected_features)} features for modeling\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 6. Prepare Training Data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: PREPARE TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X = df[selected_features]\n",
    "y = df['target']\n",
    "\n",
    "# Check class balance\n",
    "print(\"Class Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Train-test split (temporal split for time series)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Testing set:  {X_test.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 7. Hyperparameter Tuning with GridSearchCV\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define the parameter grid based on your previous results\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [20, 30, 40, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create GridSearchCV\n",
    "print(\"Performing GridSearchCV... (this may take a few minutes)\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',  # Using F1-weighted as per your requirement\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Grid search completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score (F1-weighted): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "optimized_random_forest_model = grid_search.best_estimator_\n",
    "\n",
    "# %% [markdown]\n",
    "# # 8. Model Evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = optimized_random_forest_model.predict(X_test)\n",
    "y_pred_proba = optimized_random_forest_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"ðŸ”¢ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': optimized_random_forest_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Top 15 Feature Importances:\")\n",
    "print(\"=\" * 40)\n",
    "for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "    print(f\"{i:2}. {row['feature']:25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # 9. Cross-Validation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 8: CROSS-VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform cross-validation on the entire dataset\n",
    "cv_scores = cross_val_score(\n",
    "    optimized_random_forest_model,\n",
    "    X,\n",
    "    y,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"Cross-Validation Scores (F1-weighted): {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std CV Score:  {cv_scores.std():.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 10. Save Model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 9: SAVE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the optimized model\n",
    "joblib.dump(optimized_random_forest_model, 'optimized_random_forest_model.pkl')\n",
    "print(\"âœ… Model saved as 'optimized_random_forest_model.pkl'\")\n",
    "\n",
    "# Also save the feature names\n",
    "model_metadata = {\n",
    "    'model': optimized_random_forest_model,\n",
    "    'features': selected_features,\n",
    "    'feature_importance': feature_importance.to_dict(),\n",
    "    'performance_metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_metadata, 'model_metadata.pkl')\n",
    "print(\"âœ… Model metadata saved as 'model_metadata.pkl'\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 11. MLflow Integration (Optional)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 10: MLFLOW INTEGRATION (OPTIONAL)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    \n",
    "    # Set up MLflow\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    mlflow.set_experiment(\"stock_prediction\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'cv_mean_score': cv_scores.mean()\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(optimized_random_forest_model, \"model\")\n",
    "        \n",
    "        # Log feature importance\n",
    "        feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importance.csv\")\n",
    "        \n",
    "        print(\"âœ… Experiment logged to MLflow\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  MLflow not installed. Skipping MLflow logging.\")\n",
    "    print(\"   Install with: pip install mlflow\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 12. Make Predictions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 11: MAKE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the saved model for prediction\n",
    "loaded_model = joblib.load('optimized_random_forest_model.pkl')\n",
    "\n",
    "# Create sample new data for prediction\n",
    "print(\"Making predictions on new data...\")\n",
    "\n",
    "# Take last row from test set as example\n",
    "sample_data = X_test.iloc[-1:].copy()\n",
    "prediction = loaded_model.predict(sample_data)\n",
    "prediction_proba = loaded_model.predict_proba(sample_data)\n",
    "\n",
    "print(f\"\\nðŸ“Š Sample Prediction:\")\n",
    "print(f\"  Features: {sample_data.iloc[0].to_dict()}\")\n",
    "print(f\"  Prediction: {'UP' if prediction[0] == 1 else 'DOWN'}\")\n",
    "print(f\"  Probability: {prediction_proba[0]}\")\n",
    "\n",
    "# Generate trading signal\n",
    "if prediction_proba[0][1] > 0.7:\n",
    "    signal = \"STRONG BUY\"\n",
    "    confidence = \"HIGH\"\n",
    "elif prediction_proba[0][1] > 0.6:\n",
    "    signal = \"BUY\"\n",
    "    confidence = \"MEDIUM\"\n",
    "elif prediction_proba[0][1] < 0.4:\n",
    "    signal = \"SELL\"\n",
    "    confidence = \"HIGH\"\n",
    "elif prediction_proba[0][1] < 0.5:\n",
    "    signal = \"WEAK SELL\"\n",
    "    confidence = \"MEDIUM\"\n",
    "else:\n",
    "    signal = \"HOLD\"\n",
    "    confidence = \"LOW\"\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Trading Signal: {signal}\")\n",
    "print(f\"   Confidence: {confidence}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 13. Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORKFLOW COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nâœ… Model trained successfully!\")\n",
    "print(f\"ðŸ“Š Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved Files:\")\n",
    "print(f\"   1. optimized_random_forest_model.pkl - Trained model\")\n",
    "print(f\"   2. model_metadata.pkl - Model metadata and feature importance\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"   1. Use the model for real-time predictions\")\n",
    "print(f\"   2. Set up a FastAPI service for serving predictions\")\n",
    "print(f\"   3. Create a Streamlit dashboard for monitoring\")\n",
    "print(f\"   4. Implement backtesting framework\")\n",
    "\n",
    "print(f\"\\nðŸ Workflow completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
